2025/02/23 17:48:25 routes.go:1186: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/jovyan/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-02-23T17:48:25.809Z level=INFO source=images.go:432 msg="total blobs: 18"
time=2025-02-23T17:48:25.814Z level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-02-23T17:48:25.815Z level=INFO source=routes.go:1237 msg="Listening on 127.0.0.1:11434 (version 0.5.11)"
time=2025-02-23T17:48:25.816Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-02-23T17:48:26.035Z level=INFO source=types.go:130 msg="inference compute" id=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda variant=v11 compute=8.6 driver=11.8 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="23.5 GiB"
time=2025-02-23T17:49:11.054Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-23T17:49:11.173Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="491.9 GiB" free_swap="7.6 GiB"
time=2025-02-23T17:49:11.174Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-23T17:49:11.175Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 46687"
time=2025-02-23T17:49:11.177Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-23T17:49:11.177Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-23T17:49:11.177Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-23T17:49:11.216Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-23T17:49:11.216Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-23T17:49:11.217Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:46687"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-23T17:49:11.431Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-23T17:49:15.200Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/23 - 17:49:36 | 200 | 25.562201377s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:50:07 | 200 | 30.944362934s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:50:07.464Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-23T17:50:07.584Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-23T17:50:07.584Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2126053376 required="809.9 MiB"
time=2025-02-23T17:50:07.689Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="490.9 GiB" free_swap="7.6 GiB"
time=2025-02-23T17:50:07.689Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-23T17:50:07.690Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 37115"
time=2025-02-23T17:50:07.691Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-23T17:50:07.691Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-23T17:50:07.692Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-23T17:50:07.736Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-23T17:50:07.736Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-23T17:50:07.736Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:37115"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-23T17:50:07.944Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1935 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-23T17:50:08.195Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/23 - 17:50:09 | 200 |  1.589145791s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:09.057Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:09 | 200 |   73.617212ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:50:43 | 200 | 34.510263736s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:50:43.719Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |   31.434966ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.752Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |   26.477553ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.787Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |   26.245245ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.816Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |   24.644598ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.850Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |     27.0337ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.879Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:43 | 200 |   25.113983ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:50:43.912Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:50:44 | 200 |  116.210478ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:51:13 | 200 | 29.690557097s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:51:35 | 200 | 21.365603131s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:51:35.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:51:35 | 200 |   87.546076ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:51:35.325Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:51:35 | 200 |   88.132285ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:52:03 | 200 | 27.957873729s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:52:03.433Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:52:03 | 200 |  128.127628ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:52:22 | 200 | 18.501615202s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:52:51 | 200 | 29.563639627s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:52:51.766Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:52:51 | 200 |  165.013276ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:52:51.939Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:52:52 | 200 |  168.547257ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:53:36 | 200 | 44.759463292s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:53:36.988Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   41.314629ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.033Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   24.356483ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.068Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   24.200786ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.096Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   25.270258ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.133Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   20.568557ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.155Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   25.599027ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.194Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   24.283829ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.220Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   25.772981ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.258Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   27.840445ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.288Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   30.886066ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.333Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   22.257786ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.357Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   21.678268ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.390Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |   21.920855ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.414Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |    22.98489ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:53:37.447Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:53:37 | 200 |  148.944729ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:53:56 | 200 | 18.694317393s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:54:18 | 200 | 22.403266607s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:54:18.840Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:54:18 | 200 |  108.444827ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:54:18.953Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:54:19 | 200 |  101.169053ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:54:47 | 200 | 28.692954274s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:54:47.819Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:54:47 | 200 |   94.751577ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:55:10 | 200 | 22.577922721s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:55:30 | 200 | 20.412824559s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:55:31.001Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:55:31 | 200 |   99.982657ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:55:31.108Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:55:31 | 200 |  102.965299ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:56:02 | 200 |  31.54347321s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:56:02.844Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:02 | 200 |   33.647353ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:56:02.879Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:02 | 200 |   29.754932ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:56:02.917Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:03 | 200 |   86.522175ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:56:31 | 200 | 28.630050039s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:56:31.701Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:31 | 200 |   29.624188ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:56:31.734Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:31 | 200 |   27.229728ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:56:31.777Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:56:31 | 200 |    75.95847ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:56:50 | 200 | 18.416533609s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:57:08 | 200 | 18.383324878s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:57:08.795Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:08 | 200 |  108.784643ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:08.909Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:09 | 200 |  101.853514ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:57:38 | 200 | 29.935510696s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:57:39.042Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   28.000053ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.070Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   30.145131ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.110Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   21.863267ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.133Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   29.075023ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.169Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   24.225162ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.195Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   23.869387ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   26.181143ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.261Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |   20.962103ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:57:39.291Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:57:39 | 200 |  119.124371ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:58:02 | 200 | 23.390536672s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 17:58:33 | 200 | 31.007427814s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:58:33.951Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:58:34 | 200 |  249.556112ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:58:34.207Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:58:34 | 200 |  230.122928ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:59:23 | 200 | 48.869306504s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:59:23.458Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   34.660685ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.493Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |     28.2369ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.536Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   28.299997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.566Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |    27.49847ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.605Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   26.968723ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.633Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   19.865601ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.670Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |    24.05865ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.696Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |    28.80941ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.739Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   23.516039ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.764Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   22.063813ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.789Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   20.759368ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.812Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   25.633055ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.858Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   30.311002ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.893Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   16.650863ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.923Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   21.595109ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.947Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:23 | 200 |   19.726875ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:23.974Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   31.175691ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.008Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   27.182986ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.057Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   24.562351ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.085Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   25.000911ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.126Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   31.221029ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.159Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   24.955872ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.197Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   25.577984ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.233Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   25.073645ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.262Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   22.187039ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.286Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   20.263091ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.327Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   25.029507ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.353Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   24.836628ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.379Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   22.396215ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.403Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   29.080638ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.454Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   25.142309ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.482Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   26.601997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.522Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   20.854642ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.546Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   28.508064ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.575Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |    17.74268ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.595Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |   41.238579ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T17:59:24.656Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:24 | 200 |  218.003361ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 17:59:53 | 200 |  28.22694567s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T17:59:53.182Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 17:59:53 | 200 |    98.01935ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:00:18 | 200 | 25.118202556s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:00:18.506Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   32.633653ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.540Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |    25.25136ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.580Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   27.712961ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.610Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   25.558439ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.660Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   25.701683ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.687Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   26.510853ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:00:18.727Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:00:18 | 200 |   73.423724ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:01:09 | 200 | 50.369610695s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:01:09.356Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   38.298107ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.396Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   31.919721ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.443Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   23.997952ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.470Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   24.652787ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.507Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   24.244138ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.534Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |   22.449533ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:01:09.570Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:01:09 | 200 |  140.942443ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:01:34 | 200 | 24.251919141s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:02:13 | 200 | 39.196934582s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:02:13.622Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:02:13 | 200 |  135.237846ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:02:13.761Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:02:13 | 200 |  109.892816ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:02:46 | 200 | 32.304174426s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:02:46.302Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:02:46 | 200 |   25.142677ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:02:46.328Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:02:46 | 200 |   24.953413ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:02:46.366Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:02:46 | 200 |   83.016182ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:03:03 | 200 |  17.46500466s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:03:29 | 200 | 25.394541799s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:03:29.463Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:03:29 | 200 |   117.55461ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:03:29.585Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:03:29 | 200 |  112.980776ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:04:06 | 200 | 37.248406959s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:04:07.068Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |    30.98292ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.102Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   25.620036ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.137Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   31.494533ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.171Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   23.681705ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.202Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   26.179789ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   23.814797ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.263Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   26.453782ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.291Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   22.469557ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.321Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   22.379292ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.344Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   23.664964ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.378Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |    29.50372ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.410Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   23.857922ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.444Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   21.856494ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.469Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |   22.137637ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:07.499Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:07 | 200 |  101.780033ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:04:28 | 200 |   20.9478877s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:04:54 | 200 | 25.654488841s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:04:54.327Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:54 | 200 |  119.740973ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:04:54.450Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:04:54 | 200 |  112.782936ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:05:27 | 200 | 32.908046089s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:05:27.559Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:27 | 200 |   25.130379ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:27.585Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:27 | 200 |   23.777828ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:27.620Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:27 | 200 |   28.243923ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:27.651Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:27 | 200 |   23.314424ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:27.683Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:27 | 200 |  117.675528ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:05:59 | 200 | 31.228514803s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:05:59.111Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |   30.990446ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.147Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |   31.544103ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.193Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |   31.457424ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.227Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |    24.82909ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.265Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |   30.610416ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.297Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |   28.457186ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:05:59.336Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:05:59 | 200 |  126.528688ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:06:32 | 200 | 33.443053751s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:06:33.030Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:06:33 | 200 |  115.016412ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:07:04 | 200 | 31.334672435s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:07:04.603Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |    26.42951ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.632Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   25.894395ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.670Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |    24.39951ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.697Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   26.958704ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.739Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   30.178167ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.771Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   26.406217ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.811Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   23.868193ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.837Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |   23.946701ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:04.872Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:04 | 200 |  117.898229ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:07:25 | 200 | 20.385894579s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:07:44 | 200 | 18.507153626s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:07:44.160Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:44 | 200 |   85.563999ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:07:44.250Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:07:44 | 200 |   79.184403ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:08:12 | 200 |  27.81600745s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:08:12.221Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   31.665623ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.257Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   26.286844ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.294Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   24.193689ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.320Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   22.995456ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.353Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   21.808179ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.378Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |   32.553892ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:12.419Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:12 | 200 |  101.420492ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:08:28 | 200 |  15.58412654s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:08:42 | 200 | 14.592265391s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:08:42.782Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:42 | 200 |   82.167605ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:08:42.865Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:08:42 | 200 |   62.763723ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:09:06 | 200 | 23.899717565s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:09:06.902Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:09:06 | 200 |     64.5162ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:09:31 | 200 |  24.11410292s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:09:49 | 200 |  17.95945595s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:09:49.152Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:09:49 | 200 |  114.769152ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:09:49.270Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:09:49 | 200 |   98.282981ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:10:30 | 200 | 41.088851737s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:10:30.554Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:10:30 | 200 |  159.508907ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:10:48 | 200 | 17.753630653s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:11:10 | 200 |  21.94695967s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:11:10.541Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:11:10 | 200 |   73.571046ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:11:10.619Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:11:10 | 200 |   76.119032ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:11:40 | 200 | 29.516776623s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:11:40.314Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:11:40 | 200 |    67.91638ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:11:53 | 200 | 13.017441139s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:12:09 | 200 | 15.713722972s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:12:09.198Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:12:09 | 200 |   93.805245ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:12:09.297Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:12:09 | 200 |   95.984967ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:12:34 | 200 | 24.723964397s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:12:34.201Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:12:34 | 200 |   84.523124ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:12:34.343Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2091 keep=5 new=2048
[GIN] 2025/02/23 - 18:13:01 | 200 | 26.747018075s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:13:23 | 200 | 22.765315263s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:13:23.955Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:13:24 | 200 |   86.617018ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:13:24.046Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:13:24 | 200 |   68.928688ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:13:46 | 200 | 22.171869456s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:13:46.351Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:13:46 | 200 |   38.968839ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:13:46.394Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:13:46 | 200 |   25.946303ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:13:46.431Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:13:46 | 200 |   51.296591ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:14:05 | 200 | 19.401422714s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/23 - 18:14:38 | 200 | 32.730503388s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:14:38.794Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:14:38 | 200 |  140.339971ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-23T18:14:38.937Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:14:39 | 200 |  130.228739ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/23 - 18:15:16 | 200 | 37.591980142s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-23T18:15:16.768Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/23 - 18:15:16 | 200 |  122.578722ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:36:10.040Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T01:36:10.151Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="489.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T01:36:10.151Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T01:36:10.152Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 38483"
time=2025-02-24T01:36:10.153Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T01:36:10.153Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T01:36:10.155Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T01:36:10.188Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T01:36:10.188Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T01:36:10.189Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38483"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T01:36:10.408Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
time=2025-02-24T01:36:11.361Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server not responding"
time=2025-02-24T01:36:11.612Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T01:36:14.878Z level=INFO source=server.go:596 msg="llama runner started in 4.72 seconds"
[GIN] 2025/02/24 - 01:36:30 | 200 | 21.140135025s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 01:37:50 | 200 | 21.703998543s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T01:37:50.818Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T01:37:50.994Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T01:37:50.994Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2126053376 required="809.9 MiB"
time=2025-02-24T01:37:51.115Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.2 GiB" free_swap="7.6 GiB"
time=2025-02-24T01:37:51.115Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T01:37:51.116Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 41245"
time=2025-02-24T01:37:51.117Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T01:37:51.117Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T01:37:51.118Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T01:37:51.160Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T01:37:51.160Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T01:37:51.161Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:41245"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T01:37:51.370Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1935 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T01:37:51.621Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 01:37:52 | 200 |  1.790073053s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:37:52.610Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:37:52 | 200 |   75.957241ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 01:38:15 | 200 | 22.943952021s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T01:38:15.721Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   33.654496ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.757Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   23.776998ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.792Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   21.993816ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.815Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   24.605314ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.850Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   21.184528ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.872Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   24.809077ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.908Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   22.835963ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.931Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:15 | 200 |   21.431046ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T01:38:15.961Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 01:38:16 | 200 |   82.751615ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T02:51:58.849Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T02:51:58.988Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="489.0 GiB" free_swap="7.6 GiB"
time=2025-02-24T02:51:58.990Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T02:51:58.991Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 32965"
time=2025-02-24T02:51:58.993Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T02:51:58.993Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T02:51:58.995Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T02:51:59.044Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T02:51:59.044Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T02:51:59.045Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:32965"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T02:51:59.247Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T02:52:06.282Z level=INFO source=server.go:596 msg="llama runner started in 7.29 seconds"
[GIN] 2025/02/24 - 02:52:25 | 200 | 26.963798613s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T02:57:32.157Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T02:57:32.267Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T02:57:32.269Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T02:57:32.270Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 35809"
time=2025-02-24T02:57:32.272Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T02:57:32.273Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T02:57:32.274Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T02:57:32.320Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T02:57:32.320Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T02:57:32.321Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:35809"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T02:57:32.526Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T02:57:37.299Z level=INFO source=server.go:596 msg="llama runner started in 5.03 seconds"
[GIN] 2025/02/24 - 02:57:56 | 200 | 24.687554804s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 02:58:42 | 200 | 20.143790579s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 03:00:51 | 200 | 20.093370953s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T03:23:27.103Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T03:23:27.236Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.8 GiB" free_swap="7.6 GiB"
time=2025-02-24T03:23:27.240Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T03:23:27.241Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 44217"
time=2025-02-24T03:23:27.252Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T03:23:27.252Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T03:23:27.253Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T03:23:27.296Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T03:23:27.296Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T03:23:27.297Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:44217"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T03:23:27.509Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
time=2025-02-24T03:23:28.463Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server not responding"
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
time=2025-02-24T03:23:28.800Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T03:23:31.565Z level=INFO source=server.go:596 msg="llama runner started in 4.31 seconds"
[GIN] 2025/02/24 - 03:23:50 | 200 | 24.041415583s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 03:27:12 | 200 | 20.069558574s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 03:29:35 | 200 | 21.967446515s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T05:45:57.814Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T05:45:57.958Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="489.1 GiB" free_swap="7.6 GiB"
time=2025-02-24T05:45:57.959Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T05:45:57.960Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 45921"
time=2025-02-24T05:45:57.962Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T05:45:57.963Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T05:45:57.964Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T05:45:58.008Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T05:45:58.008Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T05:45:58.009Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:45921"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T05:45:58.216Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
time=2025-02-24T05:46:02.185Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server not responding"
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T05:46:02.437Z level=INFO source=server.go:596 msg="llama runner started in 4.47 seconds"
[GIN] 2025/02/24 - 05:46:24 | 200 | 26.973179488s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 05:50:51 | 200 | 20.656888758s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:01:32.517Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T06:01:32.644Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:01:32.645Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T06:01:32.646Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 36345"
time=2025-02-24T06:01:32.649Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T06:01:32.649Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:01:32.650Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:01:32.688Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:01:32.688Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:01:32.689Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:36345"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:01:32.912Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T06:01:37.184Z level=INFO source=server.go:596 msg="llama runner started in 4.53 seconds"
[GIN] 2025/02/24 - 06:02:00 | 200 | 27.923714344s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:06:31 | 200 | 20.754199955s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:08:19 | 200 | 19.289208622s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:12:04 | 200 | 21.946444472s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:12:27 | 200 | 21.910665839s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:17:05 | 200 | 21.986916308s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:17:27 | 200 | 21.905320737s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:20:50 | 200 | 21.974461751s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:21:12 | 200 | 21.854469362s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:22:57 | 200 | 22.004514874s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:23:19 | 200 | 21.950321451s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:25:31 | 200 | 19.110342329s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:29:58.073Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T06:29:58.206Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T06:29:58.206Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2121859072 required="809.9 MiB"
time=2025-02-24T06:29:58.315Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.6 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:29:58.315Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T06:29:58.316Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42741"
time=2025-02-24T06:29:58.317Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T06:29:58.318Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:29:58.318Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:29:58.364Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:29:58.364Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:29:58.365Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42741"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:29:58.570Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1931 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T06:29:58.822Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 06:29:59 | 200 |  1.903576681s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:29:59.980Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:00 | 200 |   67.338488ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:30:27 | 200 | 27.167096242s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:30:27.334Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |    31.31444ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.368Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   20.114327ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.408Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   17.895042ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.430Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   26.905003ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.468Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   26.032812ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.499Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   27.780755ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:30:27.539Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:30:27 | 200 |   79.523749ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:31:09 | 200 | 19.956281054s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:32:12.084Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:32:12 | 200 |   107.34356ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:32:12.192Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:32:12 | 200 |   68.136208ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:33:47 | 200 |         1m34s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:40:36.195Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T06:40:36.319Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:40:36.320Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T06:40:36.320Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 38915"
time=2025-02-24T06:40:36.323Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T06:40:36.323Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:40:36.324Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:40:36.364Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:40:36.364Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:40:36.365Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38915"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:40:36.576Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T06:40:41.602Z level=INFO source=server.go:596 msg="llama runner started in 5.28 seconds"
[GIN] 2025/02/24 - 06:41:04 | 200 | 28.708192343s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:41:04.756Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T06:41:04.890Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T06:41:04.890Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2167996416 required="809.9 MiB"
time=2025-02-24T06:41:04.994Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.8 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:41:04.994Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T06:41:04.996Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 34657"
time=2025-02-24T06:41:04.997Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T06:41:04.997Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:41:04.997Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:41:05.040Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:41:05.040Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:41:05.041Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:34657"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:41:05.252Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1975 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T06:41:05.503Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 06:41:06 | 200 |  1.542312468s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:41:06.304Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:41:06 | 200 |   64.995493ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:42:41 | 200 |         1m34s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:43:22 | 200 | 40.131094458s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:43:22.763Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |   35.933226ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.798Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |   21.537625ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.826Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |   23.566639ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.852Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |   18.209219ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.877Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |   21.279692ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.900Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:22 | 200 |    24.28038ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:43:22.932Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:43:23 | 200 |  113.800539ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:53:20.326Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T06:53:20.442Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:53:20.442Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T06:53:20.444Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 35279"
time=2025-02-24T06:53:20.446Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T06:53:20.446Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:53:20.448Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:53:20.488Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:53:20.488Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:53:20.488Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:35279"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:53:20.702Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T06:53:24.470Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/24 - 06:53:47 | 200 | 27.499655395s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 06:54:20 | 200 | 33.188860388s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:54:21.075Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T06:54:21.193Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T06:54:21.193Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2126053376 required="809.9 MiB"
time=2025-02-24T06:54:21.300Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.8 GiB" free_swap="7.6 GiB"
time=2025-02-24T06:54:21.300Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T06:54:21.302Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 39965"
time=2025-02-24T06:54:21.304Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T06:54:21.304Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T06:54:21.304Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T06:54:21.348Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T06:54:21.348Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T06:54:21.349Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:39965"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T06:54:21.557Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1935 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T06:54:21.809Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 06:54:22 | 200 |  1.626778881s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:22.706Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:22 | 200 |  137.973715ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:54:50 | 200 | 27.355414199s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:54:50.266Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   29.521545ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.299Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   25.311437ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.338Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   30.961633ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.371Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   21.717829ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.404Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   32.528934ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.440Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   27.132016ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.480Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   22.313994ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.503Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   26.009869ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.540Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   25.076193ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.567Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |   20.569822ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:54:50.598Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:54:50 | 200 |  103.548301ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 06:55:29 | 200 | 38.439862081s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T06:55:29.250Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   28.134997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.276Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   19.589541ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.306Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   25.408702ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.333Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   23.519972ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.365Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   25.309626ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.474Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   27.205187ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.517Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   24.846213ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.545Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   30.292864ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.590Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   31.346568ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.624Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   25.308685ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.659Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   29.533844ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.692Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   25.770224ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.726Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   27.059971ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.755Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   26.686925ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.794Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   27.402118ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.822Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |   22.243596ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T06:55:29.853Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 06:55:29 | 200 |  102.519444ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:02:25.651Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T07:02:25.765Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T07:02:25.767Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T07:02:25.768Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 36661"
time=2025-02-24T07:02:25.769Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T07:02:25.769Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:02:25.769Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:02:25.812Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:02:25.812Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:02:25.813Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:36661"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:02:26.022Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T07:02:30.295Z level=INFO source=server.go:596 msg="llama runner started in 4.53 seconds"
[GIN] 2025/02/24 - 07:02:50 | 200 | 24.574231811s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:03:01 | 200 | 10.973915658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:04:22 | 200 | 18.495539409s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:05:40 | 200 | 19.464992066s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:15:44.671Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T07:15:44.794Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.6 GiB" free_swap="7.6 GiB"
time=2025-02-24T07:15:44.795Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T07:15:44.795Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 42833"
time=2025-02-24T07:15:44.797Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T07:15:44.797Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:15:44.799Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:15:44.840Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:15:44.840Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:15:44.840Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42833"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:15:45.051Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T07:15:49.072Z level=INFO source=server.go:596 msg="llama runner started in 4.28 seconds"
time=2025-02-24T07:15:49.103Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2081 keep=5 new=2048
[GIN] 2025/02/24 - 07:16:16 | 200 | 32.294593374s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:17:52.072Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:18:12 | 200 | 20.840485821s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:18:31.307Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:18:53 | 200 | 22.750431052s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:22:15 | 200 | 21.132980088s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:22:15.515Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T07:22:15.645Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T07:22:15.646Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T07:22:15.794Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.7 GiB" free_swap="7.6 GiB"
time=2025-02-24T07:22:15.795Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T07:22:15.795Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 41487"
time=2025-02-24T07:22:15.797Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T07:22:15.797Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:22:15.797Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:22:15.844Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:22:15.844Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:22:15.844Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:41487"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:22:16.050Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T07:22:16.301Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 07:22:17 | 200 |  1.636633253s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:17.157Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:17 | 200 |  108.873867ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:22:43 | 200 | 26.092343678s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:22:43.473Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   31.770303ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.506Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   21.075056ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.536Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   24.587234ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.563Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   18.399623ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.591Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   25.609982ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.619Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   22.925799ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.651Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   25.454008ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.677Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   22.667935ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.709Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   20.998214ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.731Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   22.647851ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.764Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   23.697705ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.788Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   16.965989ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.814Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   20.321127ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.836Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |   22.876475ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.871Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |    25.89099ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.899Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:43 | 200 |    25.27766ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:22:43.936Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:22:44 | 200 |   98.067089ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:23:06 | 200 |  22.20874175s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:23:06.296Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   27.727492ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.325Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   23.502177ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.358Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   30.539071ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.391Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   31.796308ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.431Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   30.293056ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.464Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   19.535761ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.494Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   24.898858ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.521Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   23.708317ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:06.557Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:06 | 200 |   49.629699ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:23:37 | 200 | 30.780344072s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:23:37.481Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:37 | 200 |   35.596701ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:37.522Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:37 | 200 |   27.416799ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:23:37.558Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:23:37 | 200 |  112.256571ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:30:11.122Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T07:30:11.240Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.7 GiB" free_swap="7.5 GiB"
time=2025-02-24T07:30:11.241Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T07:30:11.242Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 38683"
time=2025-02-24T07:30:11.243Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T07:30:11.243Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:30:11.244Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:30:11.296Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:30:11.296Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:30:11.296Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38683"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:30:11.497Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T07:30:15.769Z level=INFO source=server.go:596 msg="llama runner started in 4.53 seconds"
[GIN] 2025/02/24 - 07:30:49 | 200 |  38.67948669s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:30:49.696Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T07:30:49.827Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T07:30:49.827Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2167996416 required="809.9 MiB"
time=2025-02-24T07:30:49.952Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.6 GiB" free_swap="7.5 GiB"
time=2025-02-24T07:30:49.952Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T07:30:49.953Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 40679"
time=2025-02-24T07:30:49.955Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T07:30:49.955Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:30:49.955Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:30:50.004Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:30:50.004Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:30:50.005Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:40679"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:30:50.207Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1975 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T07:30:50.458Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 07:30:51 | 200 |  1.618975202s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:30:51.319Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:30:51 | 200 |  103.225744ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:31:17 | 200 | 26.007716572s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:31:17.530Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |     31.8234ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.563Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |    21.79002ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.594Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   31.458205ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.629Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   25.178166ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.665Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   23.232747ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.690Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   19.930978ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.721Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   23.998214ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.748Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |    29.70273ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.788Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   22.871595ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.812Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   26.836792ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:17.848Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:17 | 200 |   84.037508ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:31:32 | 200 | 14.800103797s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:31:32.804Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:32 | 200 |   38.998733ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:31:52 | 200 | 19.711833194s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:31:52.608Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:52 | 200 |   26.354181ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:52.636Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:52 | 200 |   25.435033ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:52.675Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:52 | 200 |   33.109598ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:52.708Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:52 | 200 |   24.828858ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:31:52.741Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:31:52 | 200 |   56.263034ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:32:09 | 200 |  16.61613688s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:32:09.473Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   34.064421ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.508Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   26.410512ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.540Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   32.445759ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.576Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   20.577076ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.605Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   24.095229ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.631Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   22.385198ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:32:09.660Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:32:09 | 200 |   46.406218ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:33:55.487Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:34:18 | 200 | 22.665357522s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:34:44 | 200 | 25.592605444s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:34:44.107Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:34:44 | 200 |  147.366855ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:34:44.264Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:34:44 | 200 |  148.150946ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:35:11 | 200 | 27.194126086s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:35:11.675Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:35:11 | 200 |  132.348214ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:35:35 | 200 |  23.40997983s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:35:35.282Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:35:35 | 200 |   65.481625ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:36:15 | 200 | 40.338074693s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:36:15.829Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:15 | 200 |   29.064716ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:15.861Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:15 | 200 |   28.091246ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:15.898Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:15 | 200 |   26.955236ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:15.928Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:15 | 200 |   22.076492ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:15.969Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:15 | 200 |   26.604208ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:15.997Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |   20.144192ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:16.028Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |   24.883444ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:16.055Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |   28.445593ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:16.170Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |    23.57961ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:16.194Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |   29.420024ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:36:16.235Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:36:16 | 200 |  131.446281ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:37:01 | 200 | 45.163334268s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:37:01.739Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   43.743383ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.787Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   25.731132ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.823Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   22.953733ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.848Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   25.850123ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.885Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   26.948707ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.913Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   18.296125ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.933Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   24.964366ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:01.962Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:01 | 200 |   26.724592ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.006Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   32.522597ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.037Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   27.707136ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.074Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   24.725172ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.099Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   20.006879ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.124Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   24.318438ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.150Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   25.323603ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.184Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   22.974965ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.208Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   18.007818ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.227Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   21.254467ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.251Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   23.441855ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.290Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   25.194444ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.318Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   22.632911ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.351Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   25.584885ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.379Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   23.990509ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.413Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |    19.48398ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.434Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   22.910076ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.461Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   19.967938ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.483Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   23.249081ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.521Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   22.112727ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.545Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |   17.363404ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:37:02.575Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:37:02 | 200 |  123.249485ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:45:35.474Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T07:45:35.588Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.6 GiB" free_swap="7.5 GiB"
time=2025-02-24T07:45:35.589Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T07:45:35.590Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 37835"
time=2025-02-24T07:45:35.592Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T07:45:35.592Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:45:35.593Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:45:35.636Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:45:35.636Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:45:35.637Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:37835"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:45:35.846Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T07:45:39.864Z level=INFO source=server.go:596 msg="llama runner started in 4.27 seconds"
time=2025-02-24T07:45:39.897Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:46:07 | 200 | 32.413780297s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:46:26 | 200 |  18.24425609s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:47:12.458Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:47:35 | 200 | 22.650657285s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:47:55 | 200 | 20.231304415s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:48:17.929Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 07:48:40 | 200 |  22.75280138s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 07:49:01 | 200 | 20.280874422s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:49:01.239Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T07:49:01.393Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T07:49:01.394Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T07:49:01.507Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T07:49:01.507Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T07:49:01.508Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 39555"
time=2025-02-24T07:49:01.511Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T07:49:01.511Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T07:49:01.511Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T07:49:01.556Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T07:49:01.556Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T07:49:01.557Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:39555"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T07:49:01.764Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T07:49:02.014Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 07:49:02 | 200 |  1.747905761s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:02.994Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:03 | 200 |   152.50891ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:49:45 | 200 | 42.779096331s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:49:46.043Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   26.189898ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.073Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   24.176924ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.113Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   27.814327ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.143Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   23.484107ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.179Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   26.948296ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.209Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |    19.76365ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.240Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   30.436909ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.272Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   22.254247ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.308Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |    26.58162ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.336Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   24.574817ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.368Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   22.324965ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.392Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |   24.355213ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:49:46.435Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:49:46 | 200 |  117.300967ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:50:10 | 200 | 24.179690001s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:50:10.796Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:10 | 200 |   33.085431ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:10.830Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:10 | 200 |   31.096877ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:10.872Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:10 | 200 |   29.069738ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:10.906Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:10 | 200 |   25.412496ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.056Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   27.579819ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.087Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   18.612465ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.107Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   22.241329ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.132Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   25.036457ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.173Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   24.169523ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.201Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   25.194514ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:50:11.238Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:11 | 200 |   80.752308ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:50:56 | 200 | 45.127334084s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:50:56.590Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:50:56 | 200 |  198.111216ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 07:51:22 | 200 | 25.415028683s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T07:51:22.339Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:51:22 | 200 |   35.277216ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:51:22.375Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:51:22 | 200 |   29.544601ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T07:51:22.416Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 07:51:22 | 200 |    98.30683ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:03:10.093Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T08:03:10.213Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.6 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:03:10.213Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T08:03:10.214Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 44671"
time=2025-02-24T08:03:10.215Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T08:03:10.215Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:03:10.216Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:03:10.260Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:03:10.260Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:03:10.261Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:44671"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:03:10.469Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T08:03:14.236Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-24T08:03:14.258Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 08:03:42 | 200 | 32.156710728s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:04:00 | 200 | 18.237075249s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:04:00.760Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T08:04:00.908Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T08:04:00.908Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T08:04:01.068Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.4 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:04:01.069Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T08:04:01.071Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 38627"
time=2025-02-24T08:04:01.073Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T08:04:01.073Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:04:01.073Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:04:01.116Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:04:01.116Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:04:01.117Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38627"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:04:01.326Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T08:04:01.577Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 08:04:02 | 200 |  1.686137309s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:02.449Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:02 | 200 |  101.328517ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:04:39 | 200 | 37.136517288s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:04:39.785Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   35.195179ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.821Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   29.678725ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.855Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   23.251614ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.881Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   24.578576ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.914Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   26.342295ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.944Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |    27.51505ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:39.978Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:39 | 200 |   23.981484ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.004Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   19.893393ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.034Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   24.713978ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.060Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   26.999999ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.094Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   24.786665ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.122Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   24.368355ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:04:40.156Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:04:40 | 200 |   128.49954ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:05:17 | 200 | 37.149156159s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:05:17.558Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:17 | 200 |   31.194296ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:05:45 | 200 | 27.690033246s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:05:45.358Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |    30.97049ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.386Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |   21.982265ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.417Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |   23.741293ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.444Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |   21.369957ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.472Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |   25.339885ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.499Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |   25.937646ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:05:45.533Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:05:45 | 200 |    79.62704ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:12:42.873Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T08:12:42.986Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:12:42.986Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T08:12:42.987Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 39873"
time=2025-02-24T08:12:42.989Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T08:12:42.989Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:12:42.989Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:12:43.044Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:12:43.044Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:12:43.045Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:39873"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:12:43.242Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T08:12:47.012Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-24T08:12:47.043Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 08:13:14 | 200 | 32.336867604s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:13:47 | 200 | 31.983435932s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:13:47.322Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T08:13:47.465Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T08:13:47.466Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T08:13:47.578Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.4 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:13:47.578Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T08:13:47.578Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 46333"
time=2025-02-24T08:13:47.586Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T08:13:47.586Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:13:47.586Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:13:47.607Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:13:47.607Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:13:47.607Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:46333"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
time=2025-02-24T08:13:47.843Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T08:13:48.094Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 08:13:49 | 200 |  1.734950217s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:13:49.062Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:13:49 | 200 |  166.165492ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:14:27 | 200 |  38.69533735s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:14:28.048Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   36.515446ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.085Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |    23.97167ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.119Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   27.543073ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.150Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   30.619049ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.194Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   27.432998ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.223Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |    21.84423ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.258Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   24.132337ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.285Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   20.089078ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.328Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   26.208191ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.354Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |    17.50529ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.392Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   22.841388ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.418Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |   29.926057ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:28.462Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:28 | 200 |  151.202882ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:14:58 | 200 |  30.30696496s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:14:58.996Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:59 | 200 |   36.374535ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:59.034Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:59 | 200 |    23.47842ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:14:59.070Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:14:59 | 200 |  115.671155ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:15:25 | 200 | 25.830341438s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:23:58.086Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T08:23:58.212Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:23:58.213Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T08:23:58.214Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 33035"
time=2025-02-24T08:23:58.219Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T08:23:58.219Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:23:58.219Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:23:58.260Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:23:58.260Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:23:58.261Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:33035"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:23:58.472Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T08:24:02.493Z level=INFO source=server.go:596 msg="llama runner started in 4.27 seconds"
[GIN] 2025/02/24 - 08:24:24 | 200 | 27.095341628s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:33:02.978Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T08:33:03.097Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:33:03.099Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T08:33:03.101Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 40029"
time=2025-02-24T08:33:03.102Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T08:33:03.102Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:33:03.103Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:33:03.148Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:33:03.148Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:33:03.149Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:40029"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:33:03.357Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T08:33:07.377Z level=INFO source=server.go:596 msg="llama runner started in 4.27 seconds"
time=2025-02-24T08:33:07.398Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2073 keep=5 new=2048
[GIN] 2025/02/24 - 08:33:35 | 200 | 32.385537837s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:37:36 | 200 | 17.547321044s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:37:36.962Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T08:37:37.098Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T08:37:37.099Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T08:37:37.211Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:37:37.211Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T08:37:37.212Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 37363"
time=2025-02-24T08:37:37.214Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T08:37:37.214Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:37:37.214Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:37:37.260Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:37:37.260Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:37:37.261Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:37363"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:37:37.467Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T08:37:37.719Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 08:37:38 | 200 |  1.640483734s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:38.607Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:38 | 200 |  120.046051ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:37:59 | 200 | 21.044871515s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:37:59.827Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |   29.129036ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:59.854Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |    24.94681ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:59.892Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |   24.795129ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:59.919Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |   23.718948ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:59.953Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |   24.524468ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:37:59.979Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:37:59 | 200 |   20.017502ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:00.009Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:00 | 200 |   24.880891ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:00.036Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:00 | 200 |   41.830684ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:00.085Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:00 | 200 |  102.667136ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:38:34 | 200 | 34.232175021s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:38:34.505Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   40.007516ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.546Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   26.670675ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.585Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   33.006098ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.616Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   23.812166ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.652Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   30.474955ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.684Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   44.875379ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.741Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   29.409581ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.772Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   24.575367ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.804Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   25.573428ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.830Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   21.133141ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.863Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   24.914086ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.890Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:34 | 200 |   21.092318ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:34.920Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:35 | 200 |  127.584063ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:38:56 | 200 | 21.124280035s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:38:56.247Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:56 | 200 |   32.350987ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:56.280Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:56 | 200 |   27.979447ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:56.319Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:56 | 200 |   26.628037ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:56.346Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:56 | 200 |   25.131007ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:38:56.389Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:38:56 | 200 |    63.22026ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:39:34 | 200 |  37.67145581s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:39:34.268Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:39:34 | 200 |   46.884278ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:39:34.313Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:39:34 | 200 |   58.931637ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:39:34.381Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:39:34 | 200 |   25.213537ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:39:34.408Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:39:34 | 200 |    25.11182ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:39:34.443Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:39:34 | 200 |   111.39657ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:44:05 | 200 |  38.07121095s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:47:14 | 200 | 15.499935613s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:47:54 | 200 | 15.507886728s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:49:19 | 200 |  12.05229746s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:49:19.244Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T08:49:19.385Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T08:49:19.385Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T08:49:19.495Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:49:19.496Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T08:49:19.497Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42097"
time=2025-02-24T08:49:19.500Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T08:49:19.500Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:49:19.500Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:49:19.552Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:49:19.552Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:49:19.553Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42097"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:49:19.755Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T08:49:20.006Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 08:49:20 | 200 |  1.559582311s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:49:20.803Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:49:20 | 200 |   29.935429ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:49:35 | 200 | 14.878329687s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:49:35.770Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:49:35 | 200 |   29.227611ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:50:35.356Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2085 keep=5 new=2048
[GIN] 2025/02/24 - 08:51:10 | 200 | 35.152636422s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:52:15.652Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2050 keep=5 new=2048
[GIN] 2025/02/24 - 08:52:33 | 200 | 17.916785623s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:55:15 | 200 | 29.332218001s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 08:56:20 | 200 | 23.370160077s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:56:20.866Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T08:56:20.988Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T08:56:20.988Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T08:56:21.098Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T08:56:21.098Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T08:56:21.101Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 32913"
time=2025-02-24T08:56:21.102Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T08:56:21.102Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T08:56:21.102Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T08:56:21.148Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T08:56:21.148Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T08:56:21.149Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:32913"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T08:56:21.355Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T08:56:21.606Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 08:56:22 | 200 |  1.714069467s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:22.582Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:22 | 200 |  127.237131ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:56:51 | 200 | 28.443759917s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:56:51.230Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   29.631069ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.259Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   24.177988ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.293Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   21.840146ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.320Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   25.502615ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.360Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   22.594805ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.385Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |    20.89023ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.409Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   30.252129ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.444Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   21.973672ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.483Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   27.432756ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.512Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   28.824451ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.559Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   29.292935ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.591Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   28.725034ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.631Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   22.464673ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.655Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   27.901686ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:56:51.698Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:56:51 | 200 |   83.160711ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:57:15 | 200 | 24.207124703s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:57:16.061Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:16 | 200 |   32.157834ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:16.090Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:16 | 200 |   25.946879ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:16.135Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:16 | 200 |   27.830264ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:16.164Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:16 | 200 |   21.934037ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:16.204Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:16 | 200 |   50.550534ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:57:59 | 200 | 42.752359967s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:57:59.140Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   32.695045ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.172Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   21.387058ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.210Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   23.272519ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.234Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   22.934679ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.271Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   27.792511ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.302Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |   21.516528ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:57:59.337Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:57:59 | 200 |  121.277609ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 08:58:35 | 200 | 35.677537512s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T08:58:35.265Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   27.848068ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.292Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   26.415045ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.328Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   24.871168ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.356Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   19.995038ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.388Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |     28.5512ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.417Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   23.130368ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.452Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   27.162014ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.480Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   19.713995ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T08:58:35.513Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 08:58:35 | 200 |   94.301685ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:00:19.390Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:00:40 | 200 | 21.484621652s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:06:53.508Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T09:06:53.640Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.1 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:06:53.641Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T09:06:53.641Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 37573"
time=2025-02-24T09:06:53.643Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T09:06:53.643Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:06:53.645Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:06:53.688Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:06:53.688Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:06:53.689Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:37573"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:06:53.898Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T09:06:57.920Z level=INFO source=server.go:596 msg="llama runner started in 4.28 seconds"
time=2025-02-24T09:06:57.946Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:07:27 | 200 | 33.854639466s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:07:33 | 200 |  5.658598764s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:07:37.430Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:08:01 | 200 | 24.068961037s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:08:30 | 200 | 28.293104999s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:08:30.497Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T09:08:30.644Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T09:08:30.644Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T09:08:30.751Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.9 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:08:30.751Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T09:08:30.753Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42311"
time=2025-02-24T09:08:30.754Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T09:08:30.754Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:08:30.754Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:08:30.800Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:08:30.800Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:08:30.801Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42311"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:08:31.007Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T09:08:31.258Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 09:08:32 | 200 |  1.755167791s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:08:32.259Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:08:32 | 200 |  165.268237ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:09:02 | 200 | 29.759640576s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:09:02.261Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:02 | 200 |   29.038298ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:09:02.289Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:02 | 200 |    31.22194ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:09:02.332Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:02 | 200 |  102.261381ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:09:48 | 200 | 46.446030647s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:09:49.018Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:49 | 200 |   33.019641ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:09:49.052Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:49 | 200 |   32.139682ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:09:49.098Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:09:49 | 200 |  162.283454ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:10:22 | 200 | 32.854994482s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:10:22.238Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:10:22 | 200 |   27.641523ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:10:22.265Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:10:22 | 200 |   27.451869ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:10:22.308Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:10:22 | 200 |   98.264437ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:11:06 | 200 | 44.390507416s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:11:07.068Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:11:07 | 200 |   47.046153ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:11:07.116Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:11:07 | 200 |   26.440238ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:11:07.165Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:11:07 | 200 |    26.09955ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:11:07.190Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:11:07 | 200 |    22.31601ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:11:07.223Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:11:07 | 200 |  116.571822ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:18:39.978Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T09:18:40.095Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:18:40.096Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T09:18:40.097Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 36655"
time=2025-02-24T09:18:40.099Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T09:18:40.099Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:18:40.100Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:18:40.144Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:18:40.144Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:18:40.145Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:36655"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:18:40.352Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T09:18:44.122Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-24T09:18:44.142Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:19:13 | 200 | 33.506727003s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:19:28 | 200 | 15.383926193s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:19:29.019Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T09:19:29.194Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T09:19:29.194Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T09:19:29.299Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:19:29.300Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T09:19:29.301Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42663"
time=2025-02-24T09:19:29.302Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T09:19:29.302Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:19:29.303Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:19:29.340Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:19:29.340Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:19:29.341Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42663"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:19:29.555Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T09:19:29.807Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 09:19:30 | 200 |  1.587855743s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:19:30.606Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:30 | 200 |   44.828968ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:19:46 | 200 | 15.474153149s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:19:46.170Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:46 | 200 |   29.599308ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:19:46.197Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:46 | 200 |   18.871998ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:19:46.224Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:46 | 200 |   31.101887ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:19:46.257Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:46 | 200 |   27.946721ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:19:46.291Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:19:46 | 200 |   34.522228ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:22:50.526Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:23:14 | 200 | 24.035740791s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:23:34 | 200 | 19.292831094s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:23:34.157Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:23:34 | 200 |  119.946444ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:23:34.282Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:23:34 | 200 |  137.145263ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:24:11 | 200 | 37.368994795s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:24:11.907Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:24:12 | 200 |  113.818874ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:24:39 | 200 | 27.105363361s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:24:39.217Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:24:39 | 200 |   96.654012ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:25:10 | 200 |  31.18495719s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:25:10.637Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:25:10 | 200 |   125.07402ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:28:07.936Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:28:32 | 200 | 24.116601969s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:28:51 | 200 |  19.26892384s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:28:51.751Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:28:51 | 200 |   39.344162ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:28:51.790Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:28:51 | 200 |   36.259296ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:29:05 | 200 | 14.023940493s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:29:05.889Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:29:05 | 200 |   38.149847ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:30:56.677Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2063 keep=5 new=2048
[GIN] 2025/02/24 - 09:31:20 | 200 | 24.049212965s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:31:49 | 200 | 28.153697423s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:31:49.174Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:31:49 | 200 |    57.84031ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:31:49.234Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:31:49 | 200 |   44.935108ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:32:04 | 200 | 15.525929659s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:32:04.844Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:32:04 | 200 |   27.789529ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:32:04.870Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:32:04 | 200 |   28.021023ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:32:04.903Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:32:04 | 200 |   26.394531ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:32:04.930Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:32:04 | 200 |   22.492355ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:32:04.954Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:32:04 | 200 |   31.801749ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:42:13.135Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T09:42:13.291Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:42:13.293Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T09:42:13.293Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 38093"
time=2025-02-24T09:42:13.294Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T09:42:13.294Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:42:13.296Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:42:13.332Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:42:13.332Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:42:13.333Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38093"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:42:13.548Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T09:42:17.317Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-24T09:42:17.340Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2353 keep=5 new=2048
[GIN] 2025/02/24 - 09:42:55 | 200 | 42.391656612s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:43:17.888Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2353 keep=5 new=2048
[GIN] 2025/02/24 - 09:43:52 | 200 | 34.565536747s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:44:11 | 200 | 18.816416121s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:46:56.313Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2363 keep=5 new=2048
[GIN] 2025/02/24 - 09:47:30 | 200 | 34.300488641s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:47:40.114Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2342 keep=5 new=2048
[GIN] 2025/02/24 - 09:48:22 | 200 | 42.379316975s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 09:48:43 | 200 | 20.669647461s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:48:43.880Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T09:48:44.035Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T09:48:44.036Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T09:48:44.151Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.9 GiB" free_swap="7.5 GiB"
time=2025-02-24T09:48:44.151Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T09:48:44.152Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 38337"
time=2025-02-24T09:48:44.154Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T09:48:44.154Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T09:48:44.154Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T09:48:44.192Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T09:48:44.192Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T09:48:44.193Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38337"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T09:48:44.406Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T09:48:44.657Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 09:48:45 | 200 |  1.605360021s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T09:48:45.487Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:48:45 | 200 |   68.190316ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:49:02 | 200 | 16.650043183s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:49:02.250Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 09:49:02 | 200 |    80.10395ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 09:49:13 | 200 |  11.47264886s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:50:24.698Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2354 keep=5 new=2048
[GIN] 2025/02/24 - 09:51:02 | 200 | 38.163340656s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:51:17.488Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2354 keep=5 new=2048
[GIN] 2025/02/24 - 09:51:55 | 200 | 38.298421897s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:55:11.024Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2345 keep=5 new=2048
[GIN] 2025/02/24 - 09:56:01 | 200 | 50.174940886s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:57:27.744Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2348 keep=5 new=2048
[GIN] 2025/02/24 - 09:58:12 | 200 | 44.985174888s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T09:59:43.664Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2350 keep=5 new=2048
[GIN] 2025/02/24 - 10:00:14 | 200 | 31.363474333s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 10:00:51 | 200 | 36.530256845s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:00:52.055Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T10:00:52.222Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T10:00:52.222Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T10:00:52.332Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.9 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:00:52.332Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T10:00:52.333Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 34093"
time=2025-02-24T10:00:52.335Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T10:00:52.335Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:00:52.335Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:00:52.372Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:00:52.372Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:00:52.373Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:34093"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:00:52.587Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T10:00:52.839Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 10:00:53 | 200 |   1.61537549s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:00:53.668Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:00:53 | 200 |     48.5756ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:01:16 | 200 | 22.279351973s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:01:16.069Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:01:16 | 200 |   29.831352ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:01:16.099Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:01:16 | 200 |   23.214918ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:01:16.129Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:01:16 | 200 |   23.777297ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:01:16.154Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:01:16 | 200 |   26.135765ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:01:16.185Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:01:16 | 200 |   44.438438ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:01:23.358Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2351 keep=5 new=2048
[GIN] 2025/02/24 - 10:01:54 | 200 | 31.648875107s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:02:28.976Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2341 keep=5 new=2048
[GIN] 2025/02/24 - 10:03:00 | 200 | 31.552577264s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:05:16.158Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2353 keep=5 new=2048
[GIN] 2025/02/24 - 10:05:47 | 200 | 31.347666081s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:06:49.210Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2358 keep=5 new=2048
[GIN] 2025/02/24 - 10:07:20 | 200 | 31.430369765s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:13:14.550Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T10:13:14.714Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.0 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:13:14.716Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T10:13:14.716Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 35579"
time=2025-02-24T10:13:14.718Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T10:13:14.718Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:13:14.719Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:13:14.756Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:13:14.756Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:13:14.757Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:35579"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:13:14.973Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T10:13:18.741Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-24T10:13:18.761Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2358 keep=5 new=2048
[GIN] 2025/02/24 - 10:13:55 | 200 |  40.97771987s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:18:15.726Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2358 keep=5 new=2048
[GIN] 2025/02/24 - 10:18:47 | 200 | 31.422651353s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:23:20.847Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2358 keep=5 new=2048
[GIN] 2025/02/24 - 10:23:52 | 200 | 31.358165063s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:25:41.250Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2358 keep=5 new=2048
[GIN] 2025/02/24 - 10:26:12 | 200 | 31.401478216s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:26:36.425Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2386 keep=5 new=2048
[GIN] 2025/02/24 - 10:27:17 | 200 | 40.719660749s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:29:04.850Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2148 keep=5 new=2048
[GIN] 2025/02/24 - 10:29:30 | 200 |  25.40357443s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:33:38.392Z level=WARN source=runner.go:129 msg="truncating input prompt" limit=2048 prompt=2148 keep=5 new=2048
[GIN] 2025/02/24 - 10:34:03 | 200 | 25.417986199s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 10:38:36 | 200 | 24.020102907s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:38:36.798Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T10:38:36.933Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T10:38:36.933Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-24T10:38:37.047Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.8 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:38:37.047Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T10:38:37.047Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 46769"
time=2025-02-24T10:38:37.050Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T10:38:37.050Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:38:37.051Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:38:37.100Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:38:37.100Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:38:37.100Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:46769"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:38:37.303Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T10:38:37.554Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 10:38:38 | 200 |  1.553931186s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:38:38.354Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:38:38 | 200 |   52.900351ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:39:04 | 200 | 26.579215792s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:39:05.075Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   33.469351ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.109Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   22.386067ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.146Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   23.406138ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.173Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   23.979953ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.207Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   22.821498ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.234Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   24.132208ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:39:05.267Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:39:05 | 200 |   73.132585ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:44:14.060Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T10:44:14.183Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.9 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:44:14.186Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T10:44:14.186Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 43823"
time=2025-02-24T10:44:14.188Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T10:44:14.188Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:44:14.188Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:44:14.228Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:44:14.228Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:44:14.229Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:43823"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:44:14.441Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T10:44:18.207Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/24 - 10:44:44 | 200 | 30.510143345s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 10:45:06 | 200 | 20.961607242s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:45:06.671Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T10:45:06.799Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T10:45:06.799Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2117664768 required="809.9 MiB"
time=2025-02-24T10:45:06.907Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.8 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:45:06.907Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T10:45:06.908Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 38493"
time=2025-02-24T10:45:06.910Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T10:45:06.910Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:45:06.910Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:45:06.952Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:45:06.952Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:45:06.954Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38493"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:45:07.162Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1927 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T10:45:07.413Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 10:45:08 | 200 |  1.569095756s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:08.240Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:08 | 200 |   49.142619ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:45:27 | 200 |  19.53127793s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:45:27.883Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:27 | 200 |   33.030326ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:27.913Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:27 | 200 |   19.576478ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:27.937Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:27 | 200 |   26.034823ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:27.964Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:27 | 200 |   19.119593ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:27.989Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:28 | 200 |   27.756575ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:28.017Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:28 | 200 |   25.126855ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:45:28.048Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:45:28 | 200 |   50.275328ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:47:32 | 200 | 24.074257541s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 10:47:53 | 200 | 20.352243564s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:47:53.189Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:47:53 | 200 |  115.465367ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:47:53.311Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:47:53 | 200 |  101.034344ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:48:17 | 200 | 23.609376732s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:48:17.104Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:48:17 | 200 |   65.696589ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:48:41 | 200 | 24.015245865s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:48:41.237Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:48:41 | 200 |   29.205912ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:48:41.270Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:48:41 | 200 |   24.796392ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:48:41.301Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:48:41 | 200 |   84.560477ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:51:42 | 200 | 19.162174338s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:51:42.266Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:51:42 | 200 |   98.091997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:51:42.371Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:51:42 | 200 |   86.750932ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:52:05 | 200 | 23.496580037s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:52:06.014Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:52:06 | 200 |   74.764567ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:52:30 | 200 | 23.943276992s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:52:30.088Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:52:30 | 200 |   27.767639ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:52:30.114Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:52:30 | 200 |   19.133783ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:52:30.143Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:52:30 | 200 |      85.121ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:57:10 | 200 | 23.933597503s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 10:57:30 | 200 | 20.328863528s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:57:30.924Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T10:57:31.043Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T10:57:31.043Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T10:57:31.160Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.7 GiB" free_swap="7.5 GiB"
time=2025-02-24T10:57:31.160Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T10:57:31.162Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42421"
time=2025-02-24T10:57:31.163Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T10:57:31.163Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T10:57:31.164Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T10:57:31.200Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T10:57:31.200Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T10:57:31.201Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42421"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T10:57:31.418Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T10:57:31.670Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 10:57:32 | 200 |   1.61986991s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:57:32.548Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:57:32 | 200 |   85.303154ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:57:56 | 200 |  23.54501808s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:57:56.235Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:57:56 | 200 |   87.818688ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:58:18 | 200 | 21.801667193s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:58:18.185Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:18 | 200 |   76.968865ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:58:44 | 200 | 26.064286726s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:58:44.394Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |    29.40173ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.426Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   29.320916ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.464Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   30.940135ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.495Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   17.253584ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.521Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   24.251083ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.548Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   25.935584ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:58:44.579Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:58:44 | 200 |   73.336257ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 10:59:06 | 200 | 21.668015518s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T10:59:06.477Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:59:06 | 200 |   31.265133ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:59:06.507Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:59:06 | 200 |    18.34127ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:59:06.535Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:59:06 | 200 |   22.903861ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:59:06.559Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:59:06 | 200 |   20.527759ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T10:59:06.589Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 10:59:06 | 200 |     52.6836ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:08:03.128Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T11:08:03.252Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.8 GiB" free_swap="7.5 GiB"
time=2025-02-24T11:08:03.253Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T11:08:03.254Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 46813"
time=2025-02-24T11:08:03.256Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T11:08:03.256Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T11:08:03.256Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T11:08:03.292Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T11:08:03.292Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T11:08:03.293Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:46813"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T11:08:03.509Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T11:08:07.278Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/24 - 11:08:27 | 200 | 24.577376397s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 11:08:48 | 200 | 20.251243647s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:08:48.126Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T11:08:48.253Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T11:08:48.253Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T11:08:48.360Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="486.8 GiB" free_swap="7.5 GiB"
time=2025-02-24T11:08:48.360Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T11:08:48.360Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 38821"
time=2025-02-24T11:08:48.363Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T11:08:48.363Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T11:08:48.363Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T11:08:48.404Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T11:08:48.404Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T11:08:48.405Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:38821"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T11:08:48.615Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T11:08:48.867Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 11:08:49 | 200 |  1.579224989s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:08:49.710Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:08:49 | 200 |   90.920113ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:09:11 | 200 | 22.153726901s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:09:12.021Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:09:12 | 200 |   66.338944ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:09:48 | 200 | 35.995626347s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:09:48.173Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:09:48 | 200 |  106.856074ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:10:09 | 200 | 21.400989747s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:10:09.758Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:09 | 200 |   39.562367ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:09.796Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:09 | 200 |   17.530414ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:09.821Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:09 | 200 |   29.555865ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:09.853Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:09 | 200 |   23.525894ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:09.884Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:09 | 200 |   69.516908ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:10:37 | 200 | 27.134410669s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:10:37.241Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:37 | 200 |    34.35467ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:37.277Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:37 | 200 |   22.385902ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:10:37.305Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:10:37 | 200 |   80.803567ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:13:13 | 200 | 24.026490255s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 11:13:34 | 200 |  20.33240751s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:13:34.498Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:13:34 | 200 |  102.874221ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:13:34.603Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:13:34 | 200 |   94.372715ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:13:56 | 200 | 21.969730646s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:13:56.731Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:13:56 | 200 |   52.559465ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:14:21 | 200 | 24.447516417s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:14:21.290Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   35.894923ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.325Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   23.862909ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.358Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   27.240063ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.387Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   22.467689ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.420Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   28.633169ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.450Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   23.062566ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.483Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   27.561997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.512Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   30.191603ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:21.548Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:21 | 200 |   69.825255ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:14:46 | 200 | 24.562949267s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:14:46.275Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   33.423838ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.308Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   15.747852ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.333Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   27.071318ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.361Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   23.536034ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.392Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   24.996619ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.419Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   21.444366ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.448Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   23.855308ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.474Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   23.824939ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:14:46.504Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:14:46 | 200 |   81.320916ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:15:11 | 200 | 24.629709564s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:15:11.301Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   32.049294ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.336Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   28.196794ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.375Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   26.122606ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.403Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   22.202603ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.434Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   47.649536ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.482Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   24.848702ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.519Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |    23.28267ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.544Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |    20.38341ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:15:11.575Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:15:11 | 200 |   76.609162ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:17:24 | 200 | 18.574725394s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 11:19:04 | 200 | 18.678079443s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 11:19:24 | 200 | 20.326635197s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:19:24.864Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:19:24 | 200 |  102.294351ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:19:24.972Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:19:25 | 200 |    94.89034ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:19:48 | 200 | 23.296961252s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:19:48.421Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:19:48 | 200 |  155.939773ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:20:14 | 200 | 25.437687298s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:20:14.076Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:20:14 | 200 |  106.363706ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:20:33 | 200 | 19.712623146s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:20:33.956Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:20:34 | 200 |   78.983295ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:20:57 | 200 | 23.588899136s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:20:57.699Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:20:57 | 200 |   81.615393ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:23:09 | 200 | 24.017561914s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 11:23:30 | 200 | 20.321566989s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:23:30.466Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:23:30 | 200 |   99.593527ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:23:30.569Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:23:30 | 200 |   89.943111ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:23:55 | 200 | 24.623393434s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:23:55.337Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:23:55 | 200 |   84.297746ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:24:17 | 200 | 21.759276436s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:24:17.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:17 | 200 |   63.915326ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:24:41 | 200 | 24.143446273s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:24:41.514Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |    65.11465ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.581Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |   25.339932ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.615Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |   27.550152ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.645Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |   19.942396ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.673Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |    28.59264ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.704Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |   22.374925ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:24:41.735Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:24:41 | 200 |   105.78255ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 11:25:16 | 200 | 34.285344437s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T11:25:16.341Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   31.380252ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.372Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   23.463098ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.407Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   21.749434ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.431Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   26.240757ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.463Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |    30.19506ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.495Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   22.072208ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.525Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   25.451717ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.552Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   26.827117ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T11:25:16.587Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 11:25:16 | 200 |   73.135642ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:06:46.311Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T12:06:46.433Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.5 GiB" free_swap="7.5 GiB"
time=2025-02-24T12:06:46.434Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T12:06:46.436Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 39287"
time=2025-02-24T12:06:46.437Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T12:06:46.437Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T12:06:46.439Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T12:06:46.480Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T12:06:46.480Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T12:06:46.481Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:39287"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T12:06:46.691Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T12:06:50.461Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/24 - 12:07:10 | 200 |  24.74114039s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 12:07:31 | 200 | 20.371315746s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:07:31.887Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T12:07:32.079Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T12:07:32.079Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T12:07:32.190Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.4 GiB" free_swap="7.5 GiB"
time=2025-02-24T12:07:32.190Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T12:07:32.193Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42323"
time=2025-02-24T12:07:32.195Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T12:07:32.195Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T12:07:32.196Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T12:07:32.228Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T12:07:32.228Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T12:07:32.229Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42323"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T12:07:32.448Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T12:07:32.699Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 12:07:33 | 200 |  1.638073943s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:07:33.525Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:07:33 | 200 |  103.725737ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:07:56 | 200 | 22.993623177s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:07:56.679Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:07:56 | 200 |   76.551105ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:08:17 | 200 | 20.960230904s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:08:17.762Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |   30.897536ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.793Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |    27.13187ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.827Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |   29.647726ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.858Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |    20.97435ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.888Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |   22.210142ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.913Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:17 | 200 |   25.129158ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:17.946Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:18 | 200 |   92.105921ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:08:37 | 200 | 19.652959196s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:08:37.771Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   39.524154ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.808Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |    22.89589ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.843Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   26.947317ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.871Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   21.687644ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.901Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   25.372923ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.927Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   22.170176ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.956Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   19.176409ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:37.976Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:37 | 200 |   17.215867ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:38.003Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:38 | 200 |   25.053893ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:38.029Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:38 | 200 |   25.055416ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:38.062Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:38 | 200 |   93.904536ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:08:57 | 200 | 19.718960174s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:08:57.956Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   67.205255ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.024Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   27.796707ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.060Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |    21.91563ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.083Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   25.617886ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.118Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |    25.70837ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.145Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   27.451539ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.184Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   21.013202ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.206Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   26.571647ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.242Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   24.058749ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.268Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   25.407408ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:08:58.301Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:08:58 | 200 |   69.331489ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:35:23.001Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T12:35:23.145Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.4 GiB" free_swap="7.5 GiB"
time=2025-02-24T12:35:23.146Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T12:35:23.147Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 42137"
time=2025-02-24T12:35:23.149Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T12:35:23.149Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T12:35:23.150Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T12:35:23.188Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T12:35:23.188Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T12:35:23.188Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:42137"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T12:35:23.403Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T12:35:27.180Z level=INFO source=server.go:596 msg="llama runner started in 4.03 seconds"
[GIN] 2025/02/24 - 12:35:47 | 200 | 24.630303625s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 12:38:12 | 200 | 20.319379543s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:38:12.940Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T12:38:13.060Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T12:38:13.060Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T12:38:13.162Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="487.4 GiB" free_swap="7.5 GiB"
time=2025-02-24T12:38:13.162Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T12:38:13.163Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 32799"
time=2025-02-24T12:38:13.167Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T12:38:13.167Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T12:38:13.167Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T12:38:13.208Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T12:38:13.209Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T12:38:13.209Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:32799"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T12:38:13.423Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T12:38:13.675Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 12:38:14 | 200 |  1.539324667s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:38:14.486Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:38:14 | 200 |    93.37291ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:38:40 | 200 |  26.09239085s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:38:40.726Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:38:40 | 200 |   40.646078ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:38:40.766Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:38:40 | 200 |   21.568201ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:38:40.795Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:38:40 | 200 |   78.239884ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:39:11 | 200 | 31.101649988s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:39:12.047Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   36.714413ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.084Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   27.116267ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.117Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   25.158144ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.144Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   22.037839ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.176Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   26.990517ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.205Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   20.249507ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:12.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:12 | 200 |   83.915279ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:39:35 | 200 | 22.888653511s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:39:35.269Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:35 | 200 |   25.608008ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:35.296Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:35 | 200 |   23.428161ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:39:35.329Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:39:35 | 200 |   69.608348ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 12:40:00 | 200 | 25.444012578s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T12:40:00.926Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:00 | 200 |   35.200769ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:00.962Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:00 | 200 |   26.297422ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.000Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   25.792392ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.027Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |    13.15937ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.047Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   24.952194ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.076Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   26.892226ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.111Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   29.077553ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.141Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |    20.70736ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.170Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   26.642166ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.199Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   24.496794ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T12:40:01.232Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 12:40:01 | 200 |   73.776843ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:03:43.488Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T13:03:43.607Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="490.6 GiB" free_swap="7.6 GiB"
time=2025-02-24T13:03:43.608Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T13:03:43.609Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 35961"
time=2025-02-24T13:03:43.610Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T13:03:43.610Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T13:03:43.610Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T13:03:43.652Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T13:03:43.652Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T13:03:43.653Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:35961"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T13:03:43.863Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T13:03:47.883Z level=INFO source=server.go:596 msg="llama runner started in 4.27 seconds"
[GIN] 2025/02/24 - 13:04:08 | 200 | 24.874359828s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 13:04:28 | 200 |  20.31608555s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T13:04:28.708Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T13:04:28.839Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T13:04:28.839Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T13:04:28.947Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="489.2 GiB" free_swap="7.6 GiB"
time=2025-02-24T13:04:28.947Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T13:04:28.950Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 35301"
time=2025-02-24T13:04:28.951Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T13:04:28.951Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T13:04:28.952Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T13:04:28.988Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T13:04:28.988Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T13:04:28.988Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:35301"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T13:04:29.204Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T13:04:29.456Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 13:04:30 | 200 |  1.606215473s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:30.318Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:30 | 200 |   90.623474ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 13:04:59 | 200 | 29.060445632s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T13:04:59.544Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |   36.564594ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.577Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |   18.261829ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.606Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |   25.610995ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.633Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |   17.340334ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.661Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |    25.85375ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.690Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |   22.553506ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:04:59.721Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:04:59 | 200 |    65.62678ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 13:05:41 | 200 |  41.25398608s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T13:05:41.143Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   34.520968ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.178Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |    24.57378ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.210Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   25.275037ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.237Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   18.958325ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.263Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   26.238993ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.290Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   19.972686ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.317Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |    20.81003ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.339Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   21.362345ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.370Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   25.751471ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.397Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   17.222411ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.416Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   23.570674ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.443Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   26.807691ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.484Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   26.967344ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.511Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |   21.570173ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:05:41.545Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:05:41 | 200 |  117.848903ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 13:06:03 | 200 | 21.320963776s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T13:06:03.057Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:03 | 200 |   25.513199ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:03.082Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:03 | 200 |   22.809852ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:03.115Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:03 | 200 |   23.553123ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:03.141Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:03 | 200 |   21.527659ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:03.171Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:03 | 200 |   46.356806ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 13:06:43 | 200 | 40.667567313s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T13:06:44.012Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |    34.03149ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.046Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   34.189739ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.084Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   83.978576ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.171Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   24.949932ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.203Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |    28.79545ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.234Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   24.552282ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.266Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   21.866972ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.290Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   22.218924ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.321Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   22.695449ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.345Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   23.060137ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.377Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   27.815241ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.407Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |   25.577803ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T13:06:44.439Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 13:06:44 | 200 |  115.649415ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:00:29.141Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T14:00:29.275Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="490.0 GiB" free_swap="7.6 GiB"
time=2025-02-24T14:00:29.276Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T14:00:29.277Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 41009"
time=2025-02-24T14:00:29.278Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T14:00:29.278Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T14:00:29.278Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T14:00:29.316Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T14:00:29.316Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T14:00:29.317Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:41009"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T14:00:29.533Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T14:00:33.302Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
[GIN] 2025/02/24 - 14:00:53 | 200 | 24.537163589s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 14:01:14 | 200 | 20.280422428s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:01:14.168Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T14:01:14.287Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T14:01:14.287Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T14:01:14.395Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="489.0 GiB" free_swap="7.6 GiB"
time=2025-02-24T14:01:14.396Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T14:01:14.402Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 33555"
time=2025-02-24T14:01:14.403Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T14:01:14.403Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T14:01:14.403Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T14:01:14.444Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T14:01:14.444Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T14:01:14.445Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:33555"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T14:01:14.655Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T14:01:14.906Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 14:01:15 | 200 |  1.566925203s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:01:15.738Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:01:15 | 200 |   94.706674ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:01:40 | 200 | 24.996647911s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:01:40.942Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:01:41 | 200 |    68.32827ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:02:04 | 200 | 23.061691793s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:02:04.129Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   39.926004ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.165Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   26.201123ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.201Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   22.657182ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.226Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   27.925134ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.266Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   25.504835ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.291Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   28.706077ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.329Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   24.570148ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.354Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   18.762641ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.383Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   23.984125ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.409Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   22.020014ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.437Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   17.091238ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.455Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   21.372487ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:04.483Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:04 | 200 |   77.223863ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:02:42 | 200 | 38.323869916s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:02:43.066Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   32.728933ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.099Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   27.744702ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.140Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   24.625271ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.165Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   20.954796ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.195Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   20.915124ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.219Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   21.431419ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.246Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   25.899026ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.274Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   26.778443ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.309Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   21.857943ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.332Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   20.093621ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.361Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   23.136828ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.388Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |   25.870511ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:02:43.419Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:02:43 | 200 |  134.027358ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:03:08 | 200 | 25.161446012s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:03:08.821Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:03:08 | 200 |   40.736878ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:03:08.864Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:03:08 | 200 |   21.922187ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:03:08.896Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:03:08 | 200 |   26.997474ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:03:08.924Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:03:08 | 200 |   24.961137ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:03:08.957Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:03:09 | 200 |    69.40749ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:20:55.221Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-24T14:20:55.339Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="490.0 GiB" free_swap="7.6 GiB"
time=2025-02-24T14:20:55.340Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-24T14:20:55.342Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 41197"
time=2025-02-24T14:20:55.345Z level=INFO source=sched.go:449 msg="loaded runners" count=1
time=2025-02-24T14:20:55.345Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T14:20:55.346Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T14:20:55.388Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T14:20:55.388Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T14:20:55.389Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:41197"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T14:20:55.601Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-24T14:20:59.370Z level=INFO source=server.go:596 msg="llama runner started in 4.03 seconds"
[GIN] 2025/02/24 - 14:21:19 | 200 | 24.536417057s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/24 - 14:21:40 | 200 | 20.288834375s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:21:40.221Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-24T14:21:40.342Z level=INFO source=sched.go:507 msg="updated VRAM based on existing loaded models" gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-24T14:21:40.342Z level=INFO source=sched.go:714 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-f3d1aded-9af0-4dbf-e022-aa24284c2c72 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-24T14:21:40.447Z level=INFO source=server.go:100 msg="system memory" total="503.5 GiB" free="488.9 GiB" free_swap="7.6 GiB"
time=2025-02-24T14:21:40.447Z level=INFO source=memory.go:356 msg="offload to cuda" layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-24T14:21:40.447Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 39155"
time=2025-02-24T14:21:40.460Z level=INFO source=sched.go:449 msg="loaded runners" count=2
time=2025-02-24T14:21:40.460Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-24T14:21:40.462Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-24T14:21:40.500Z level=INFO source=runner.go:936 msg="starting go runner"
time=2025-02-24T14:21:40.500Z level=INFO source=runner.go:937 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-24T14:21:40.501Z level=INFO source=runner.go:995 msg="Server listening on 127.0.0.1:39155"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-24T14:21:40.714Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-24T14:21:40.966Z level=INFO source=server.go:596 msg="llama runner started in 0.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/24 - 14:21:41 | 200 |  1.568347262s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:21:41.790Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:21:41 | 200 |  120.444441ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:22:08 | 200 | 26.758290669s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:22:08.736Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:08 | 200 |    76.55452ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:22:47 | 200 | 38.684287811s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:22:47.650Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |    41.84099ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.689Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   23.463616ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.722Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   24.489623ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.748Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   20.996859ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.780Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   27.385748ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.808Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   20.956679ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.837Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   23.713787ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.863Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   29.199053ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.901Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   26.356513ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.929Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:47 | 200 |   18.665489ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-24T14:22:47.955Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:22:48 | 200 |   77.528477ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:23:12 | 200 |  24.17913386s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:23:12.382Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:23:12 | 200 |   70.434014ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/24 - 14:23:37 | 200 | 24.598615367s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-24T14:23:37.121Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/24 - 14:23:37 | 200 |   91.497288ms |       127.0.0.1 | POST     "/api/embed"
