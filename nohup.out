2025/02/28 09:52:19 routes.go:1205: INFO server config env="map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/jovyan/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NEW_ENGINE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]"
time=2025-02-28T09:52:19.455Z level=INFO source=images.go:432 msg="total blobs: 18"
time=2025-02-28T09:52:19.457Z level=INFO source=images.go:439 msg="total unused blobs removed: 0"
time=2025-02-28T09:52:19.459Z level=INFO source=routes.go:1256 msg="Listening on 127.0.0.1:11434 (version 0.5.12)"
time=2025-02-28T09:52:19.461Z level=INFO source=gpu.go:217 msg="looking for compatible GPUs"
time=2025-02-28T09:52:19.657Z level=INFO source=types.go:130 msg="inference compute" id=GPU-ca49f742-f613-044d-ecec-8234028d0607 library=cuda variant=v11 compute=8.6 driver=11.8 name="NVIDIA GeForce RTX 3090" total="23.7 GiB" available="23.5 GiB"
time=2025-02-28T09:52:53.715Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T09:52:53.715Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T09:52:53.716Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-28T09:52:53.843Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="455.8 GiB" free_swap="7.6 GiB"
time=2025-02-28T09:52:53.843Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T09:52:53.844Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T09:52:53.844Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-28T09:52:53.847Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 41407"
time=2025-02-28T09:52:53.848Z level=INFO source=sched.go:450 msg="loaded runners" count=1
time=2025-02-28T09:52:53.848Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T09:52:53.850Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T09:52:53.889Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T09:52:53.986Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T09:52:53.987Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:41407"
time=2025-02-28T09:52:54.102Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-28T09:53:45.594Z level=INFO source=server.go:596 msg="llama runner started in 51.75 seconds"
time=2025-02-28T09:53:45.621Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 09:54:14 | 200 |         1m20s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:54:19.336Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 09:54:43 | 200 | 24.538925894s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:54:48.951Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 09:55:13 | 200 | 24.669765741s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 09:55:45 | 200 | 30.922099283s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:55:45.167Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-28T09:55:45.372Z level=INFO source=sched.go:508 msg="updated VRAM based on existing loaded models" gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-28T09:55:45.372Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T09:55:45.372Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T09:55:45.372Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T09:55:45.372Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T09:55:45.373Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-28T09:55:45.480Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="454.7 GiB" free_swap="7.6 GiB"
time=2025-02-28T09:55:45.480Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T09:55:45.480Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T09:55:45.480Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T09:55:45.480Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T09:55:45.480Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-28T09:55:45.481Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 35027"
time=2025-02-28T09:55:45.482Z level=INFO source=sched.go:450 msg="loaded runners" count=2
time=2025-02-28T09:55:45.482Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T09:55:45.482Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T09:55:45.525Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T09:55:45.580Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T09:55:45.581Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:35027"
time=2025-02-28T09:55:45.735Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-28T09:55:46.993Z level=INFO source=server.go:596 msg="llama runner started in 1.51 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/28 - 09:55:47 | 200 |  2.675469091s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:55:47.811Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:55:47 | 200 |   91.112293ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 09:56:11 | 200 | 23.607473185s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:56:11.567Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   30.924824ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.598Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   23.920142ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.634Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   26.552931ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.665Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   21.251577ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.700Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   29.887977ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.733Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   22.991229ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.764Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   18.757239ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.787Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   23.081705ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.823Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   27.976614ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.854Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |    23.96824ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:56:11.889Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:56:11 | 200 |   72.895813ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:57:28.974Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 09:57:53 | 200 | 24.618196996s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 09:58:22 | 200 | 28.838150872s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:58:22.663Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:22 | 200 |   68.265952ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:22.736Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:22 | 200 |    83.57742ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 09:58:48 | 200 |   25.7129078s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T09:58:48.603Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   29.370001ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.632Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   21.741838ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.669Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   29.973352ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.701Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   21.033412ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.732Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   23.078952ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.759Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   23.843621ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.793Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   28.362196ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.825Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |   25.667266ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.862Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |    27.06997ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.893Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:48 | 200 |    24.84615ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T09:58:48.929Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 09:58:49 | 200 |    76.88654ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:05:53.147Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T10:05:53.147Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T10:05:53.147Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-28T10:05:53.268Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="455.2 GiB" free_swap="7.6 GiB"
time=2025-02-28T10:05:53.269Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T10:05:53.271Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T10:05:53.271Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-28T10:05:53.272Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 36735"
time=2025-02-28T10:05:53.273Z level=INFO source=sched.go:450 msg="loaded runners" count=1
time=2025-02-28T10:05:53.273Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T10:05:53.275Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T10:05:53.317Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T10:05:53.388Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T10:05:53.389Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:36735"
time=2025-02-28T10:05:53.526Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-28T10:05:57.295Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-28T10:05:57.319Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:06:25 | 200 | 32.374908658s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:06:37 | 200 | 12.087807362s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:08:19.975Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:08:44 | 200 | 24.606261787s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:09:13 | 200 | 28.770603146s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:09:13.784Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-28T10:09:13.908Z level=INFO source=sched.go:508 msg="updated VRAM based on existing loaded models" gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-28T10:09:13.908Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:09:13.926Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T10:09:13.926Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T10:09:13.949Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:09:13.949Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=1 available=2102984704 required="809.9 MiB"
time=2025-02-28T10:09:14.056Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="453.8 GiB" free_swap="7.6 GiB"
time=2025-02-28T10:09:14.056Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:09:14.057Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T10:09:14.057Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T10:09:14.057Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:09:14.057Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-28T10:09:14.057Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 46361"
time=2025-02-28T10:09:14.059Z level=INFO source=sched.go:450 msg="loaded runners" count=2
time=2025-02-28T10:09:14.059Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T10:09:14.059Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T10:09:14.105Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T10:09:14.144Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T10:09:14.145Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:46361"
time=2025-02-28T10:09:14.312Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1913 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-28T10:09:14.563Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/28 - 10:09:15 | 200 |  1.582742653s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:15.373Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:15 | 200 |   77.386073ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:09:43 | 200 | 27.658722024s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:09:43.180Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |    29.98586ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.211Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   26.338225ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.248Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   26.785391ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.278Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   25.432095ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.313Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   28.421656ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.342Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |    20.88775ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.377Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   27.013794ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.407Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |    20.84704ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.436Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   23.749555ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.463Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   26.989084ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:09:43.500Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:09:43 | 200 |   58.765282ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:12:18.862Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:12:43 | 200 | 24.609488835s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:13:12 | 200 | 28.813152866s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:13:12.521Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:12 | 200 |     85.0707ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:12.609Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:12 | 200 |   72.858909ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:13:35 | 200 | 22.541239588s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:13:35.284Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   30.987093ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.316Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   24.970299ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.353Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   25.283307ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.380Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |    22.47323ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.413Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   29.607751ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.445Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   19.139053ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.476Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   24.979535ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.505Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   24.815867ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.539Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   24.852846ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.568Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   26.978553ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:13:35.603Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:13:35 | 200 |   71.262025ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:20:35.806Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T10:20:35.807Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T10:20:35.807Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=4 available=25180045312 required="21.5 GiB"
time=2025-02-28T10:20:35.928Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="454.9 GiB" free_swap="7.6 GiB"
time=2025-02-28T10:20:35.928Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.key_length default=128
time=2025-02-28T10:20:35.929Z level=WARN source=ggml.go:132 msg="key not found" key=qwen2.attention.value_length default=128
time=2025-02-28T10:20:35.932Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=65 layers.offload=65 layers.split="" memory.available="[23.5 GiB]" memory.gpu_overhead="0 B" memory.required.full="21.5 GiB" memory.required.partial="21.5 GiB" memory.required.kv="2.0 GiB" memory.required.allocations="[21.5 GiB]" memory.weights.total="19.5 GiB" memory.weights.repeating="18.9 GiB" memory.weights.nonrepeating="609.1 MiB" memory.graph.full="676.0 MiB" memory.graph.partial="916.1 MiB"
time=2025-02-28T10:20:35.933Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 --ctx-size 8192 --batch-size 512 --n-gpu-layers 65 --threads 64 --parallel 4 --port 33289"
time=2025-02-28T10:20:35.934Z level=INFO source=sched.go:450 msg="loaded runners" count=1
time=2025-02-28T10:20:35.934Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T10:20:35.934Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T10:20:35.976Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T10:20:36.040Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T10:20:36.041Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:33289"
time=2025-02-28T10:20:36.186Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 23921 MiB free
llama_model_loader: loaded meta data with 26 key-value pairs and 771 tensors from /home/jovyan/.ollama/models/blobs/sha256-6150cb382311b69f09cc0f9a1b69fc029cbd742b66bb8ec531aa5ecf5c613e93 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen2
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Qwen 32B
llama_model_loader: - kv   3:                           general.basename str              = DeepSeek-R1-Distill-Qwen
llama_model_loader: - kv   4:                         general.size_label str              = 32B
llama_model_loader: - kv   5:                          qwen2.block_count u32              = 64
llama_model_loader: - kv   6:                       qwen2.context_length u32              = 131072
llama_model_loader: - kv   7:                     qwen2.embedding_length u32              = 5120
llama_model_loader: - kv   8:                  qwen2.feed_forward_length u32              = 27648
llama_model_loader: - kv   9:                 qwen2.attention.head_count u32              = 40
llama_model_loader: - kv  10:              qwen2.attention.head_count_kv u32              = 8
llama_model_loader: - kv  11:                       qwen2.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  12:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  13:                          general.file_type u32              = 15
llama_model_loader: - kv  14:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  15:                         tokenizer.ggml.pre str              = deepseek-r1-qwen
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,152064]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,152064]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  18:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  19:                tokenizer.ggml.bos_token_id u32              = 151646
llama_model_loader: - kv  20:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  22:               tokenizer.ggml.add_bos_token bool             = true
llama_model_loader: - kv  23:               tokenizer.ggml.add_eos_token bool             = false
llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...
llama_model_loader: - kv  25:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  321 tensors
llama_model_loader: - type q4_K:  385 tensors
llama_model_loader: - type q6_K:   65 tensors
llm_load_vocab: missing or unrecognized pre-tokenizer type, using: 'default'
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 22
llm_load_vocab: token to piece cache size = 0.9310 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = qwen2
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 152064
llm_load_print_meta: n_merges         = 151387
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 5120
llm_load_print_meta: n_layer          = 64
llm_load_print_meta: n_head           = 40
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 5
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 27648
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 32B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 32.76 B
llm_load_print_meta: model size       = 18.48 GiB (4.85 BPW) 
llm_load_print_meta: general.name     = DeepSeek R1 Distill Qwen 32B
llm_load_print_meta: BOS token        = 151646 '<｜begin▁of▁sentence｜>'
llm_load_print_meta: EOS token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOT token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: PAD token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: LF token         = 148848 'ÄĬ'
llm_load_print_meta: FIM PRE token    = 151659 '<|fim_prefix|>'
llm_load_print_meta: FIM SUF token    = 151661 '<|fim_suffix|>'
llm_load_print_meta: FIM MID token    = 151660 '<|fim_middle|>'
llm_load_print_meta: FIM PAD token    = 151662 '<|fim_pad|>'
llm_load_print_meta: FIM REP token    = 151663 '<|repo_name|>'
llm_load_print_meta: FIM SEP token    = 151664 '<|file_sep|>'
llm_load_print_meta: EOG token        = 151643 '<｜end▁of▁sentence｜>'
llm_load_print_meta: EOG token        = 151662 '<|fim_pad|>'
llm_load_print_meta: EOG token        = 151663 '<|repo_name|>'
llm_load_print_meta: EOG token        = 151664 '<|file_sep|>'
llm_load_print_meta: max token length = 256
llm_load_tensors: offloading 64 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 65/65 layers to GPU
llm_load_tensors:        CUDA0 model buffer size = 18508.35 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   417.66 MiB
llama_new_context_with_model: n_seq_max     = 4
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 2048
llama_new_context_with_model: n_batch       = 2048
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 64, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =  2048.00 MiB
llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     2.40 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =   696.00 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    26.01 MiB
llama_new_context_with_model: graph nodes  = 2246
llama_new_context_with_model: graph splits = 2
time=2025-02-28T10:20:39.955Z level=INFO source=server.go:596 msg="llama runner started in 4.02 seconds"
time=2025-02-28T10:20:39.983Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:21:07 | 200 | 32.359936438s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:21:36 | 200 | 28.291975991s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:21:36.566Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
time=2025-02-28T10:21:36.692Z level=INFO source=sched.go:508 msg="updated VRAM based on existing loaded models" gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 library=cuda total="23.7 GiB" available="2.0 GiB"
time=2025-02-28T10:21:36.692Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:21:36.692Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T10:21:36.692Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T10:21:36.692Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:21:36.692Z level=INFO source=sched.go:715 msg="new model will fit in available VRAM in single GPU, loading" model=/home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 gpu=GPU-ca49f742-f613-044d-ecec-8234028d0607 parallel=1 available=2105081856 required="809.9 MiB"
time=2025-02-28T10:21:36.797Z level=INFO source=server.go:97 msg="system memory" total="503.5 GiB" free="453.9 GiB" free_swap="7.6 GiB"
time=2025-02-28T10:21:36.797Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:21:36.797Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.key_length default=64
time=2025-02-28T10:21:36.797Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.value_length default=64
time=2025-02-28T10:21:36.797Z level=WARN source=ggml.go:132 msg="key not found" key=nomic-bert.attention.head_count_kv default=1
time=2025-02-28T10:21:36.797Z level=INFO source=server.go:130 msg=offload library=cuda layers.requested=-1 layers.model=13 layers.offload=13 layers.split="" memory.available="[2.0 GiB]" memory.gpu_overhead="0 B" memory.required.full="809.9 MiB" memory.required.partial="809.9 MiB" memory.required.kv="24.0 MiB" memory.required.allocations="[809.9 MiB]" memory.weights.total="240.1 MiB" memory.weights.repeating="195.4 MiB" memory.weights.nonrepeating="44.7 MiB" memory.graph.full="48.0 MiB" memory.graph.partial="48.0 MiB"
time=2025-02-28T10:21:36.798Z level=INFO source=server.go:380 msg="starting llama server" cmd="/usr/local/bin/ollama runner --model /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 --ctx-size 8192 --batch-size 512 --n-gpu-layers 13 --threads 64 --parallel 1 --port 42287"
time=2025-02-28T10:21:36.800Z level=INFO source=sched.go:450 msg="loaded runners" count=2
time=2025-02-28T10:21:36.800Z level=INFO source=server.go:557 msg="waiting for llama runner to start responding"
time=2025-02-28T10:21:36.800Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server error"
time=2025-02-28T10:21:36.841Z level=INFO source=runner.go:932 msg="starting go runner"
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3090, compute capability 8.6, VMM: yes
load_backend: loaded CUDA backend from /usr/local/lib/ollama/cuda_v11/libggml-cuda.so
load_backend: loaded CPU backend from /usr/local/lib/ollama/libggml-cpu-icelake.so
time=2025-02-28T10:21:36.900Z level=INFO source=runner.go:935 msg=system info="CPU : LLAMAFILE = 1 | CPU : LLAMAFILE = 1 | CUDA : USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | LLAMAFILE = 1 | cgo(gcc)" threads=64
time=2025-02-28T10:21:36.901Z level=INFO source=runner.go:993 msg="Server listening on 127.0.0.1:42287"
time=2025-02-28T10:21:37.052Z level=INFO source=server.go:591 msg="waiting for server to become available" status="llm server loading model"
llama_load_model_from_file: using device CUDA0 (NVIDIA GeForce RTX 3090) - 1915 MiB free
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 2048
llm_load_print_meta: n_embd           = 768
llm_load_print_meta: n_layer          = 12
llm_load_print_meta: n_head           = 12
llm_load_print_meta: n_head_kv        = 12
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 768
llm_load_print_meta: n_embd_v_gqa     = 768
llm_load_print_meta: f_norm_eps       = 1.0e-12
llm_load_print_meta: f_norm_rms_eps   = 0.0e+00
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 3072
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 0
llm_load_print_meta: pooling type     = 1
llm_load_print_meta: rope type        = 2
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 1000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 2048
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 137M
llm_load_print_meta: model ftype      = F16
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llm_load_tensors: offloading 12 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 13/13 layers to GPU
llm_load_tensors:        CUDA0 model buffer size =   216.14 MiB
llm_load_tensors:   CPU_Mapped model buffer size =    44.72 MiB
llama_new_context_with_model: n_seq_max     = 1
llama_new_context_with_model: n_ctx         = 8192
llama_new_context_with_model: n_ctx_per_seq = 8192
llama_new_context_with_model: n_batch       = 512
llama_new_context_with_model: n_ubatch      = 512
llama_new_context_with_model: flash_attn    = 0
llama_new_context_with_model: freq_base     = 1000.0
llama_new_context_with_model: freq_scale    = 1
llama_new_context_with_model: n_ctx_pre_seq (8192) > n_ctx_train (2048) -- possible training context overflow
llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 12, can_shift = 1
llama_kv_cache_init:      CUDA0 KV buffer size =   288.00 MiB
llama_new_context_with_model: KV self size  =  288.00 MiB, K (f16):  144.00 MiB, V (f16):  144.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.00 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =    23.50 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =     3.50 MiB
llama_new_context_with_model: graph nodes  = 453
llama_new_context_with_model: graph splits = 4 (with bs=512), 2 (with bs=1)
time=2025-02-28T10:21:37.303Z level=INFO source=server.go:596 msg="llama runner started in 0.50 seconds"
llama_model_loader: loaded meta data with 24 key-value pairs and 112 tensors from /home/jovyan/.ollama/models/blobs/sha256-970aa74c0a90ef7482477cf803618e776e173c007bf957f635f1015bfcfef0e6 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
llama_model_loader: - kv   8:                          general.file_type u32              = 1
llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  22:                tokenizer.ggml.cls_token_id u32              = 101
llama_model_loader: - kv  23:               tokenizer.ggml.mask_token_id u32              = 103
llama_model_loader: - type  f32:   51 tensors
llama_model_loader: - type  f16:   61 tensors
llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
llm_load_vocab: special tokens cache size = 5
llm_load_vocab: token to piece cache size = 0.2032 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = nomic-bert
llm_load_print_meta: vocab type       = WPM
llm_load_print_meta: n_vocab          = 30522
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: vocab_only       = 1
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = all F32
llm_load_print_meta: model params     = 136.73 M
llm_load_print_meta: model size       = 260.86 MiB (16.00 BPW) 
llm_load_print_meta: general.name     = nomic-embed-text-v1.5
llm_load_print_meta: BOS token        = 101 '[CLS]'
llm_load_print_meta: EOS token        = 102 '[SEP]'
llm_load_print_meta: UNK token        = 100 '[UNK]'
llm_load_print_meta: SEP token        = 102 '[SEP]'
llm_load_print_meta: PAD token        = 0 '[PAD]'
llm_load_print_meta: CLS token        = 101 '[CLS]'
llm_load_print_meta: MASK token       = 103 '[MASK]'
llm_load_print_meta: LF token         = 0 '[PAD]'
llm_load_print_meta: EOG token        = 102 '[SEP]'
llm_load_print_meta: max token length = 21
llama_model_load: vocab only - skipping tensors
[GIN] 2025/02/28 - 10:21:38 | 200 |    1.5779991s |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:21:38.150Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:21:38 | 200 |  114.311769ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:22:09 | 200 | 31.698258839s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:22:10.064Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:22:10 | 200 |   96.969642ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:22:40 | 200 | 30.468349434s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:22:40.700Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:22:40 | 200 |  123.496232ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:23:55.214Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:24:19 | 200 | 24.477730644s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:24:48 | 200 | 28.630636099s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:24:48.567Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:24:48 | 200 |   85.602319ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:24:48.658Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:24:48 | 200 |   77.994432ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:25:11 | 200 | 23.070456335s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:25:11.863Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:11 | 200 |   26.631648ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:11.893Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:11 | 200 |   22.850896ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:11.928Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:11 | 200 |   28.615015ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:11.961Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:11 | 200 |   26.010065ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:11.999Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |    24.31894ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.028Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   30.076587ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.067Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   23.445881ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.094Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   22.292252ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.125Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   21.038126ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.150Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   26.348746ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:25:12.186Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:25:12 | 200 |   67.242457ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:26:19.871Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:26:44 | 200 | 24.471841166s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:27:13 | 200 | 28.698663128s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:27:13.273Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:13 | 200 |   96.536722ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:13.377Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:13 | 200 |   91.707584ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:27:34 | 200 | 21.389305279s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:27:34.928Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:34 | 200 |   32.856341ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:34.962Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:34 | 200 |   25.307341ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.001Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   27.206033ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.032Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |    26.16724ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.069Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   29.185168ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.102Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   27.743706ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.139Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   22.690109ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.166Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   24.173177ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.202Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |    24.25561ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.229Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   23.485979ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:27:35.265Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:27:35 | 200 |   66.962512ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:28:44.199Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:29:08 | 200 | 24.441936676s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:29:35 | 200 | 27.062109098s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:30:32.819Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:30:57 | 200 | 24.439215428s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:31:26 | 200 |  28.60699033s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:31:26.411Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:31:26 | 200 |   91.087982ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:31:26.507Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:31:26 | 200 |   75.752355ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:31:50 | 200 | 23.501868807s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:32:03.021Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
[GIN] 2025/02/28 - 10:32:27 | 200 | 24.513539685s |       127.0.0.1 | POST     "/api/chat"
[GIN] 2025/02/28 - 10:32:56 | 200 | 28.673790083s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:32:56.440Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:32:56 | 200 |   84.783521ms |       127.0.0.1 | POST     "/api/embed"
time=2025-02-28T10:32:56.532Z level=WARN source=types.go:512 msg="invalid option provided" option=tfs_z
[GIN] 2025/02/28 - 10:32:56 | 200 |   82.887204ms |       127.0.0.1 | POST     "/api/embed"
[GIN] 2025/02/28 - 10:33:00 | 200 |  3.693248786s |       127.0.0.1 | POST     "/api/chat"
time=2025-02-28T10:33:03.824Z level=WARN source=runner.go:130 msg="truncating input prompt" limit=2048 prompt=2190 keep=5 new=2048
